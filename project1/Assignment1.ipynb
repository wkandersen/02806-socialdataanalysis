{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment covers material from Weeks 1–4. Think of it as a spot-check: five exercises that test important analytical and visualization skills from the first part of the course.\n",
    "\n",
    "## Formalia:\n",
    "\n",
    "<mark>Please read the **[assignment overview page](https://github.com/suneman/socialdata2026/wiki/Assignments)** carefully before proceeding.</mark> This page contains information about formatting (including formats etc.), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "***If you fail to follow these simple instructions, it will negatively impact your grade!***\n",
    "\n",
    "**Due date and time**: The assignment is due on Monday March 2nd, 2026 at 23:55. Hand in your files via DTU Learn. \n",
    "\n",
    "## Important Notes:\n",
    "**All exercises use your combined SF crime dataset** (2003–present) that you built during Week 2, along with your Personal Focus Crimes. Make sure you have those ready before you start.\n",
    "* **Hint**: The number of focus crimes are up to you, but choosing very few focus-crimes (1-5, let's say) feels like taking the easy way out to me, so to get top marks, you'll need 8 or more focus crimes (but it's not that more are better, 16 isn't necessarily better than 8).\n",
    "\n",
    "Each assignment draws directly on the weekly exercises - you should be able to pull your work directly from there.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "The same LLM guidance applies here as during the weekly exercises: use your LLM freely for coding, pandas syntax, and plot formatting. Do not use it for interpretation, reflection, or anything that asks for your own thinking. \n",
    "<br>  <br>\n",
    "If you use and LLM to cheat, you are missing out on your education ... it will mean that you're not getting all you can out of this class ... and, more importantly, that you are literally wasting this crucial time of your DTU-journey, designed to make you smarter and ready for the world.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1.1: Temporal Overview\n",
    "\n",
    "Let's start by getting the big picture of how crime has changed over the full 20+ year period covered by your combined dataset.\n",
    "\n",
    "*Draws from*: Week 1, Exercises 4.1 and 4.2 — applied here to the full merged dataset (2003–present) rather than just 2018–present.\n",
    "\n",
    "> * Using your combined dataset (2003–present), plot the total number of incidents **per year** for each of your Personal Focus Crimes. Display at least the years 2003–2025. \n",
    "> * Make sure your plot follows good visualization practices: labeled axes, a legend, a descriptive title.\n",
    "> * Identify and comment on at least two notable features in the plot — for example, long-term trends, sudden drops or spikes, or the impact of COVID-19 in 2020. For each feature, offer a possible explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./merged_sfpd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_focus = [\n",
    "    'larceny/theft',\n",
    "    'non-criminal',\n",
    "    'assault',\n",
    "    'vehicle theft',\n",
    "    'drug/narcotic',\n",
    "    'vandalism',\n",
    "    'warrants',\n",
    "    'burglary',\n",
    "    'suspicious occ'\n",
    "]\n",
    "df_pf = df[df['incident_category'].isin(personal_focus)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1.2: Crime Profiles by Police District\n",
    "\n",
    "Different parts of the city have very different crime patterns. Here we quantify that using conditional probabilities.\n",
    "\n",
    "*Draws from*: Week 3, Exercises 2.1 and 2.2.\n",
    "\n",
    "> * For each police district in your dataset, compute the **conditional crime profile**: for each of your Personal Focus Crimes, calculate\n",
    ">\n",
    ">   $$r(\\text{crime}, \\text{district}) = \\frac{P(\\text{crime} \\mid \\text{district})}{P(\\text{crime})}$$\n",
    ">\n",
    ">   A value above 1 means that crime type is *over-represented* in that district relative to the city-wide average; below 1 means it is *under-represented*.\n",
    "> * Visualize these ratios in a way that makes it easy to compare across both districts and crime types. (Simple barcharts are fine, but you may also use ideas from more complex visualization techniques, for example, a heatmap could work well here, but you're free to choose another format if you can justify it.)\n",
    "> * Pick **one district** whose profile stands out to you. Describe the pattern and offer an explanation for why that district looks the way it does. Are there geographic, demographic, or other factors that might explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_crime_district = df_pf.groupby('police_district')['incident_category'].value_counts(normalize=True).unstack().fillna(0)\n",
    "prob_crime = df_pf['incident_category'].value_counts(normalize=True)\n",
    "\n",
    "districts = sorted(df_pf['police_district'].dropna().unique().tolist())\n",
    "crimes = sorted(df_pf['incident_category'].dropna().unique().tolist())\n",
    "\n",
    "cond_probs = {crime: (prob_crime_district[crime] / prob_crime[crime]).to_dict() for crime in crimes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "cond_probs = pd.DataFrame(cond_probs)\n",
    "\n",
    "# Calculate sums for marginal bar charts\n",
    "row_sums = df_pf['police_district'].value_counts(normalize=True).sort_index()\n",
    "col_sums = prob_crime.sort_index()\n",
    "\n",
    "# Create figure with subplots using GridSpec\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = GridSpec(2, 2, width_ratios=[8, 1], height_ratios=[1, 8], \n",
    "              hspace=0.05, wspace=0.05)\n",
    "\n",
    "# Main heatmap\n",
    "ax_main = fig.add_subplot(gs[1, 0])\n",
    "im = ax_main.imshow(cond_probs.T, cmap='Blues', aspect='auto')\n",
    "ax_main.set_xticks(range(len(districts)))\n",
    "ax_main.set_xticklabels(districts, rotation=90)\n",
    "ax_main.set_yticks(range(len(crimes)))\n",
    "ax_main.set_yticklabels(crimes)\n",
    "ax_main.set_xlabel('Police District')\n",
    "ax_main.set_ylabel('Crime Category')\n",
    "\n",
    "# Text annotations\n",
    "for i in range(cond_probs.shape[0]):\n",
    "    for j in range(cond_probs.shape[1]):\n",
    "        val = cond_probs.iloc[i, j]\n",
    "        ax_main.text(i, j, f\"{val:.2f}\", ha='center', va='center', \n",
    "                    color='black', fontsize=8+(1.5*val))\n",
    "\n",
    "# Column Sums\n",
    "ax_top = fig.add_subplot(gs[0, 0], sharex=ax_main)\n",
    "ax_top.bar(range(len(districts)), row_sums, color='cornflowerblue', alpha=0.7)\n",
    "ax_top.set_ylabel('Prob')\n",
    "ax_top.tick_params(labelbottom=False)\n",
    "ax_top.spines['top'].set_visible(False)\n",
    "ax_top.spines['right'].set_visible(False)\n",
    "ax_top.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Row Sums\n",
    "ax_right = fig.add_subplot(gs[1, 1], sharey=ax_main)\n",
    "ax_right.barh(range(len(crimes)), col_sums, color='cornflowerblue', alpha=0.7)\n",
    "ax_right.set_xlabel('Prob')\n",
    "ax_right.tick_params(labelleft=False)\n",
    "ax_right.spines['top'].set_visible(False)\n",
    "ax_right.spines['right'].set_visible(False)\n",
    "ax_right.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "fig.suptitle('Conditional Probabilities of Police Districts Given Crimes', fontsize=14, y=0.98)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Above we see a visualization of the various types of crimes committed in the 11 districts of San Francisco. The data is visualized as a heatmap, in which each cell represents the conditional probability of a given crime in a given district. Thus, a value higher than one means that that crime is occurring more often than average, in the district. Above and to the right of the heatmap you see two bar charts. The top one shows the unconditional probability that if a crime occurs, it is in that given district. While the right bar chart shows the unconditional probability that if a crimes occurs, it is that crime (OBS. probabilities are only normalized with focus crimes, and not the total data set).\n",
    "\n",
    "When looking at the plot two thing become clear. Firstly, a very small amount of the crimes registered are \"ouf of sf\". This district is quite high in the \"non-criminal\" category. This suggests that SF Police is called out of the city, every now and then, to simply declare that something is perfectly legal. If it is a crime, it is most likely someone stealing something, either a car or something else. In and of itself, this is not that interesting, but considering that this is a data set for crime in San Francisco, it is curious that we have a district called \"Out of San Francisco\" and a category called \"non-criminal\".\n",
    "Moving on to SF we can see some interesting patterns. One that spring to mind is what is happening in the \"Southern\" district. This district does not spike out in any category, like we see with Tenderloin and drugs, instead it rests at a steady rate for each crime. In total, this district has the most crime, as seen in top bar chart. This suggests that criminals here are less picky at what crimes they commit, and instead try for a quantity over quality tactic. The most out-of-distribution crime in this district falls on the category \"warrants\", which in itself isn't a crime but typically suggests that a police investigation needs special permissions. Putting that together it seems that the Southern district of SF is one of the more \"harsh\" neighborhoods, and also one of the most policed. Which one causes the other, one can wonder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1.3: Visualizing Distributions\n",
    "\n",
    "This exercise asks you to recreate several classic plot types from DAOST Chapter 2 using your own crime data — putting visualization theory into practice.\n",
    "\n",
    "*Draws from*: Week 3, Exercises 5.2 and 5.3.\n",
    "\n",
    "> **Part A — Jitter plot**\n",
    "> * Pick one of your Personal Focus Crimes and a suitable time interval (somewhere between a month and 6 months, depending on how common the crime is). Create a jitter plot of the incident times during a single hour (e.g. 13:00–14:00): let time run along the $x$-axis and add vertical jitter.\n",
    "> * What does the jitter plot reveal about how times are recorded in the dataset? Are incidents clustered at certain minutes (on the hour, half hour, etc.)? What does this tell you about the precision of the data?\n",
    ">\n",
    "> **Part B — Probability plot**\n",
    "> * Using the same geographic data from Part B, create a probability plot (QQ plot) for the latitude distribution of each of your two crime types. (`scipy.stats.probplot` is your friend here.)\n",
    "> * What reference distribution are you comparing against? What would it mean if the points fell exactly on the straight line? Where does the distribution deviate from normal, and what does that deviation tell you about the geography of crime in SF?\n",
    ">\n",
    "> **Part C — Box plots of time-of-day**\n",
    "> * For each of your Personal Focus Crimes, extract the time-of-day of every incident.\n",
    "> * Create box plots showing the time-of-day distribution for all your Personal Focus Crimes side by side.\n",
    "> * What patterns do you see? Are there crimes that happen mostly at night? Mostly during business hours? For crimes that peak late at night, does the box plot handle the wrap-around at midnight well? What goes wrong?\n",
    "> * Above, feel free to use alternatives to box plots — violin plots, swarm plots, or raincloud plots — if you think they reveal more. If you do, briefly explain what the alternative shows that the box plot doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1.4: Spatial Power Law\n",
    "\n",
    "Is crime spread evenly across San Francisco, or is it concentrated in a small number of hotspots? Let's find out.\n",
    "\n",
    "*Draws from*: Week 4, Exercise 2.2.\n",
    "\n",
    "Use your **most common Personal Focus Crime** (by total incident count) for this exercise.\n",
    "\n",
    "> * **Step 1**: Divide San Francisco into a grid of approximately $100\\text{m} \\times 100\\text{m}$ cells using latitude and longitude. It is fine to ignore that the Earth isn't flat — the approximation is good enough for this purpose.\n",
    ">     * **Hint**: `np.histogram2d` works well here. Focus on points within the SF peninsula; filter out geographic outliers.\n",
    "> * **Step 2**: Count the number of incidents of your chosen crime in each grid cell (use all years of data).\n",
    "> * **Step 3**: Tally the distribution. Count how many cells have exactly $k$ incidents — call this $N(k)$ — for $k = 0, 1, 2, \\ldots$ up to the maximum.\n",
    "> * **Step 4**: Plot $(k+1)$ vs $N(k)$ on **linear axes**.\n",
    "> * **Step 5**: Plot $(k+1)$ vs $N(k)$ on **loglog axes**.\n",
    "> * **Step 6**: Does your crime follow a power-law spatial distribution? How can you tell from the loglog plot?\n",
    "> * **Step 7**: If crime is spatially concentrated in a small number of cells, what does that imply for how we interpret neighborhood-level crime statistics? Does the \"average block\" tell you anything useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "## Assignment 1.5: Regression and Correlation\n",
    "\n",
    "Which of your Personal Focus Crimes share the most similar weekly rhythm — and which are completely out of sync?\n",
    "\n",
    "*Draws from*: Week 4, Exercises 3.1, 3.2, and 3.3.\n",
    "\n",
    "Select **at least 4** of your Personal Focus Crimes for this exercise. More is fine - if you want to be closer to the actual weekly exercise.\n",
    "\n",
    "> * For each selected crime type, compute the total number of incidents for each of the **168 hours of the week** (i.e., Monday 00:00–01:00, Monday 01:00–02:00, ..., Sunday 23:00–00:00). Each crime type will give you a vector of 168 values.\n",
    ">\n",
    "> * **Scatterplot matrix**: Create a grid of pairwise scatterplots — one panel per pair of crime types, with one crime on each axis and each of the 168 hours as a point. Label each panel clearly with the two crime types being compared.\n",
    ">\n",
    "> * **Linear regression**: Using the closed-form equations from Week 4 (not a library), fit a regression line $y = ax + b$ to each pair and add it to the relevant panel:\n",
    ">   $$a = \\frac{\\sum_i x_i y_i - N\\langle x\\rangle\\langle y\\rangle}{\\sum_i x_i^2 - N\\langle x\\rangle^2}, \\qquad b = \\langle y \\rangle - a\\langle x \\rangle$$\n",
    ">\n",
    "> * **$R^2$**: Compute $R^2$ for each pair and display the value on each panel. \n",
    ">\n",
    "> * Looking at your results: which pair of crimes is **most correlated**? Which is **least correlated**? Does the answer match your intuition? Why might two crime types share a weekly rhythm — or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02806-socialdataanalysis (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
